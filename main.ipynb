{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f79151b",
   "metadata": {},
   "source": [
    "# Reinforcement Learning assignment: Worm\n",
    "### Made by Group 23:\n",
    "- Kaj de Lange, \n",
    "- Troy Dijsselbloem, \n",
    "- Julian Dinnissen, s1135596\n",
    "- Sten Nellen, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e0c7114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "import dp, mc, td\n",
    "importlib.reload(dp)\n",
    "importlib.reload(mc)\n",
    "importlib.reload(td)\n",
    "from dp import *\n",
    "from mc import *\n",
    "from td import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd354b98",
   "metadata": {},
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ce7e5b",
   "metadata": {},
   "source": [
    "_insert your abstract here_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5478f6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c66cf2b",
   "metadata": {},
   "source": [
    "# 0. Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1a1784c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state: [5.5]\n",
      "Current state: [5.]\n",
      "Current state: [4.5]\n",
      "Current state: [5.]\n",
      "Current state: [5.5]\n",
      "Current state: [6.]\n",
      "Current state: [5.5]\n",
      "Current state: [5.]\n",
      "Current state: [4.5]\n",
      "Current state: [4.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\GitHub\\SOW-BKI258-Reinforcement-Learning\\.venv\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:174: UserWarning: \u001b[33mWARN: The default seed argument in `Env.reset` should be `None`, otherwise the environment will by default always be deterministic. Actual default: seed\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\User\\Documents\\GitHub\\SOW-BKI258-Reinforcement-Learning\\.venv\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:134: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be int8, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\User\\Documents\\GitHub\\SOW-BKI258-Reinforcement-Learning\\.venv\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:158: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "c:\\Users\\User\\Documents\\GitHub\\SOW-BKI258-Reinforcement-Learning\\.venv\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:134: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be int8, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\User\\Documents\\GitHub\\SOW-BKI258-Reinforcement-Learning\\.venv\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:158: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    }
   ],
   "source": [
    "from environment import CustomEnv\n",
    "from gymnasium.envs.registration import register\n",
    "\n",
    "    \n",
    "register(id=\"worm-v0\", entry_point=\"__main__:CustomEnv\")\n",
    "\n",
    "\n",
    "#Create the environment\n",
    "env = gym.make(\"worm-v0\")\n",
    "# Interact with the environment\n",
    "obs, info = env.reset()\n",
    "for _ in range(10):\n",
    "    action = env.action_space.sample() # Random action\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3a0cb5",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b32cb8",
   "metadata": {},
   "source": [
    "_insert your introduction in this cell_\n",
    "\n",
    "_describe your environment and the problem the agent has to solve_\n",
    "\n",
    "_describe the objective of the report (e.g. comparing various RL algorithms) and how you are going to accomplish this (research question)_\n",
    "\n",
    "_don't forget to add plots/images of the environment, can be done via code cells, but also by inserting .png files into the jupyter notebook_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9625674",
   "metadata": {},
   "source": [
    "# 2. Dynamic Programming algorithms\n",
    "\n",
    "First run the Dynamic Programming algorithms (Policy Iteration and Value Iteration) by calling functions from the separate `dp.py` file, and create plots. Then, fill in the cell completing your 'report' within this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "872436f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call Policy Iteration algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6801822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call Value Iteration algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d2c20e",
   "metadata": {},
   "source": [
    "_In this cell, describe how the algorithms work, how the algorithms differ, plot results and/or policies (add more code cells!), etc._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc23e226",
   "metadata": {},
   "source": [
    "# 3. Monte Carlo algorithms\n",
    "\n",
    "First run the Monte Carlo algorithm (Monte Carlo Exploring Starts or Monte Carlo without Exploring Starts (with $\\epsilon$-greedy strategy)) by calling functions from the separate `mc.py` file, and create plots. Then, fill in the cell completing your 'report' within this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cea2c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call Monte Carlo algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963995b6",
   "metadata": {},
   "source": [
    "_In this cell, describe how the algorithms work, how the algorithms differ (compared to the one you did not code), plot results and/or policies (add more code cells!), etc._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89779fab",
   "metadata": {},
   "source": [
    "# 4. Temporal Difference algorithms\n",
    "\n",
    "First run the Temporal Difference algorithms (SARSA and Q-learning) by calling functions from the separate `td.py` file, and create plots. Then, fill in the cell completing your 'report' within this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aae27a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call SARSA algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e3b1183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call Q-learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9acd237",
   "metadata": {},
   "source": [
    "# 5. Comparison and discussion\n",
    "\n",
    "Compare different algorithms (MC and TD with plots). You donâ€™t need to plot DP alongside MC and TD since DP is not a learning algorithm. However, DP can provide the ground truth for optimal state or action values, which can serve as a reference when evaluating MC and TD. You can choose to plot any of the following: cumulative reward, root mean squared error, sample efficiency, or any other metric you think is a fair comparison.\n",
    "\n",
    "Include a discussion: what can you conclude by comparing different RL algorithms? Do they have certain strengths or limitations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9df56f1",
   "metadata": {},
   "source": [
    "_Add as many text cells as you like_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7076b635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add as many code cells as you like"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ae8b52",
   "metadata": {},
   "source": [
    "# 6. Conclusion\n",
    "\n",
    "Conclude your project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8de4509",
   "metadata": {},
   "source": [
    "_Add as many text cells as you like_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
